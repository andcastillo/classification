{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dogs vs Cats classification using simple linear regression\n",
    "By: Andr√©s M. Castillo\n",
    "\n",
    "Warning!! Real life is not thaat simple. I've selected the images to get this little introductory example working for you.\n",
    "\n",
    "Objective: Build a classification model that predicts the animal(cat or dog) in the picture. The purpose of this example is to illustrate the general machine learning setup and feature engeeniering process. For the ones who don't not know, bellow I show a picture of a real cat and a real dog\n",
    "\n",
    "Cat | Dog\n",
    "- | - \n",
    "<img src=\"data/cats/cat0.jpeg\"> |  <img src=\"data/dogs/dog0.jpeg\">\n",
    "\n",
    " Now lets start to see how many pictures we have and what is its format. But before let's install some libraries that we'll need during this example\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: failed\n",
      "\n",
      "PackagesNotFoundError: The following packages are missing from the target environment:\n",
      "  - cv2\n",
      "\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda remove -y cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--@ 1 acastillo  staff   6.4K Aug 15 18:03 data/cats/cat0.jpeg\r\n",
      "-rw-r--r--@ 1 acastillo  staff   6.8K Aug 15 17:55 data/cats/cat1.jpeg\r\n",
      "-rw-r--r--@ 1 acastillo  staff    12K Aug 15 17:55 data/cats/cat2.jpeg\r\n",
      "-rw-r--r--@ 1 acastillo  staff   5.3K Aug 15 17:55 data/cats/cat3.jpeg\r\n",
      "-rw-r--r--@ 1 acastillo  staff   5.7K Aug 15 17:55 data/cats/cat4.jpeg\r\n",
      "-rw-r--r--@ 1 acastillo  staff   5.2K Aug 15 17:56 data/cats/cat5.jpeg\r\n",
      "-rw-r--r--@ 1 acastillo  staff   5.1K Aug 15 17:56 data/cats/cat6.jpeg\r\n",
      "-rw-r--r--@ 1 acastillo  staff   9.3K Aug 15 18:03 data/cats/cat7.jpeg\r\n",
      "-rw-r--r--@ 1 acastillo  staff   7.6K Aug 15 18:03 data/cats/cat8.jpeg\r\n",
      "-rw-r--r--@ 1 acastillo  staff   7.3K Aug 15 18:03 data/cats/cat9.jpeg\r\n",
      "-rw-r--r--@ 1 acastillo  staff   6.4K Aug 15 18:02 data/dogs/dog0.jpeg\r\n",
      "-rw-r--r--@ 1 acastillo  staff   7.7K Aug 15 17:59 data/dogs/dog1.jpeg\r\n",
      "-rw-r--r--@ 1 acastillo  staff    32K Aug 15 17:59 data/dogs/dog2.jpeg\r\n",
      "-rw-r--r--@ 1 acastillo  staff   7.7K Aug 15 18:00 data/dogs/dog3.jpeg\r\n",
      "-rw-r--r--@ 1 acastillo  staff   5.7K Aug 15 18:00 data/dogs/dog4.jpeg\r\n",
      "-rw-r--r--@ 1 acastillo  staff    10K Aug 15 18:00 data/dogs/dog5.jpeg\r\n",
      "-rw-r--r--@ 1 acastillo  staff   8.3K Aug 15 18:01 data/dogs/dog6.jpeg\r\n",
      "-rw-r--r--@ 1 acastillo  staff   7.9K Aug 15 18:01 data/dogs/dog7.jpeg\r\n",
      "-rw-r--r--@ 1 acastillo  staff   9.3K Aug 15 18:01 data/dogs/dog8.jpeg\r\n",
      "-rw-r--r--@ 1 acastillo  staff   6.4K Aug 15 18:01 data/dogs/dog9.jpeg\r\n"
     ]
    }
   ],
   "source": [
    "!ls -alh data/*/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation / Preprocessing\n",
    "\n",
    "In this step we must load our dataset into main memory to start playing with it. \n",
    "\n",
    "False. In this little example perhaps that is possible, but in real life you wont be able to load all your images into main memory.\n",
    "And even if the dataset fits into memory, it is not recomended in any case to do so. In a real image analysis you must scale all the images to fit a given size. For the kind of analysis we are goin to peform, we can extract the features directly.\n",
    "\n",
    "Let's understand the feature extraction for a single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4a9d38c84749>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# construct the argument parser and parse the arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import cv3\n",
    " \n",
    "# construct the argument parser and parse the arguments\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-i\", \"--image\", required = True, help = \"data/cats/cat0.jpeg\")\n",
    "args = vars(ap.parse_args())\n",
    " \n",
    "# load the image and show it\n",
    "image = cv2.imread(args[\"image\"])\n",
    "cv2.imshow(\"image\", image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the image to grayscale and create a histogram\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "cv2.imshow(\"gray\", gray)\n",
    "hist = cv2.calcHist([gray], [0], None, [256], [0, 256])\n",
    "plt.figure()\n",
    "plt.title(\"Grayscale Histogram\")\n",
    "plt.xlabel(\"Bins\")\n",
    "plt.ylabel(\"# of Pixels\")\n",
    "plt.plot(hist)\n",
    "plt.xlim([0, 256])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A call to plt.show() displays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Now, lets compute a flattened color histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the image channels, initialize the tuple of colors,\n",
    "# the figure and the flattened feature vector\n",
    "chans = cv2.split(image)\n",
    "colors = (\"b\", \"g\", \"r\")\n",
    "plt.figure()\n",
    "plt.title(\"'Flattened' Color Histogram\")\n",
    "plt.xlabel(\"Bins\")\n",
    "plt.ylabel(\"# of Pixels\")\n",
    "features = []\n",
    " \n",
    "# loop over the image channels\n",
    "for (chan, color) in zip(chans, colors):\n",
    "\t# create a histogram for the current channel and\n",
    "\t# concatenate the resulting histograms for each\n",
    "\t# channel\n",
    "\thist = cv2.calcHist([chan], [0], None, [256], [0, 256])\n",
    "\tfeatures.extend(hist)\n",
    " \n",
    "\t# plot the histogram\n",
    "\tplt.plot(hist, color = color)\n",
    "\tplt.xlim([0, 256])\n",
    " \n",
    "# here we are simply showing the dimensionality of the\n",
    "# flattened color histogram 256 bins for each channel\n",
    "# x 3 channels = 768 total values -- in practice, we would\n",
    "# normally not use 256 bins for each channel, a choice\n",
    "# between 32-96 bins are normally used, but this tends\n",
    "# to be application dependent\n",
    "print \"flattened feature vector size: %d\" % (np.array(features).flatten().shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
